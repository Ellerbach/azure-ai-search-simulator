{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "996bd203",
   "metadata": {},
   "source": [
    "# Azure AI Search Simulator - Indexer Test\n",
    "\n",
    "This notebook demonstrates how to use **indexers** with the Azure AI Search Simulator to automatically ingest and index documents.\n",
    "\n",
    "## What This Notebook Tests\n",
    "\n",
    "1. **Data Source Creation** - Configure a local file system data source\n",
    "2. **Index Schema** - Create an index with various field types\n",
    "3. **Indexer Execution** - Run an indexer to process JSON metadata and TXT content files\n",
    "4. **Verification** - Confirm all 5 documents are indexed and searchable\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Start the Azure AI Search Simulator with HTTPS**:\n",
    "   ```bash\n",
    "   cd src/AzureAISearchSimulator.Api\n",
    "   dotnet run --urls \"https://localhost:7250\"\n",
    "   ```\n",
    "\n",
    "2. **Sample data files** should be in the `./data` folder (already provided)\n",
    "\n",
    "> ‚ö†Ô∏è **Note**: The Azure SDK requires HTTPS. The simulator must run on `https://localhost:7250`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014bad76",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a64411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-search-documents in c:\\users\\laurelle\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (11.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\laurelle\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.32.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\laurelle\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.0.0)\n",
      "Requirement already satisfied: azure-core>=1.28.0 in c:\\users\\laurelle\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from azure-search-documents) (1.38.0)\n",
      "Requirement already satisfied: azure-common>=1.1 in c:\\users\\laurelle\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from azure-search-documents) (1.1.28)\n",
      "Requirement already satisfied: isodate>=0.6.0 in c:\\users\\laurelle\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from azure-search-documents) (0.7.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\laurelle\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from azure-search-documents) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\laurelle\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\laurelle\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\laurelle\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\laurelle\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (2026.1.4)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\laurelle\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\laurelle\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\laurelle\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\laurelle\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\laurelle\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install azure-search-documents requests pandas\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import urllib3\n",
    "from pathlib import Path\n",
    "\n",
    "# Azure AI Search SDK imports\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchIndexer,\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexerDataSourceConnection,\n",
    "    FieldMapping,\n",
    "    IndexingParameters,\n",
    "    IndexingParametersConfiguration,\n",
    ")\n",
    "\n",
    "# For displaying results\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Suppress SSL warnings for local development\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbab6433",
   "metadata": {},
   "source": [
    "## 2. Initialize Azure AI Search Clients\n",
    "\n",
    "Configure connection to the local Azure AI Search Simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8901f864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to Azure AI Search Simulator at https://localhost:7250\n",
      "üìÅ Data path: C:\\Projets\\AzureAISimulator\\samples\\IndexerTestNotebook\\data\n",
      "üìÑ Found 5 JSON metadata files\n",
      "üìÑ Found 5 TXT content files\n"
     ]
    }
   ],
   "source": [
    "# Configuration for Azure AI Search Simulator\n",
    "SEARCH_ENDPOINT = \"https://localhost:7250\"\n",
    "ADMIN_API_KEY = \"admin-key-12345\"\n",
    "\n",
    "# Resource names for this test\n",
    "INDEX_NAME = \"indexer-test-docs\"\n",
    "DATA_SOURCE_NAME = \"local-test-files\"\n",
    "INDEXER_NAME = \"test-indexer\"\n",
    "\n",
    "# Path to sample data (relative to notebook location)\n",
    "DATA_PATH = Path(\"./data\").resolve()\n",
    "\n",
    "# Create credentials\n",
    "admin_credential = AzureKeyCredential(ADMIN_API_KEY)\n",
    "\n",
    "# Configure HTTP transport to skip SSL certificate validation for local development\n",
    "import requests as req_lib\n",
    "from azure.core.pipeline.transport import RequestsTransport\n",
    "\n",
    "session = req_lib.Session()\n",
    "session.verify = False\n",
    "transport = RequestsTransport(session=session, connection_verify=False)\n",
    "\n",
    "# Create clients\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=SEARCH_ENDPOINT,\n",
    "    credential=admin_credential,\n",
    "    transport=transport,\n",
    "    connection_verify=False\n",
    ")\n",
    "\n",
    "indexer_client = SearchIndexerClient(\n",
    "    endpoint=SEARCH_ENDPOINT,\n",
    "    credential=admin_credential,\n",
    "    transport=transport,\n",
    "    connection_verify=False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Connected to Azure AI Search Simulator at {SEARCH_ENDPOINT}\")\n",
    "print(f\"üìÅ Data path: {DATA_PATH}\")\n",
    "\n",
    "# List sample data files\n",
    "json_files = list(DATA_PATH.glob(\"*.json\"))\n",
    "txt_files = list(DATA_PATH.glob(\"*.txt\"))\n",
    "print(f\"üìÑ Found {len(json_files)} JSON metadata files\")\n",
    "print(f\"üìÑ Found {len(txt_files)} TXT content files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848ae11e",
   "metadata": {},
   "source": [
    "## 3. Review Sample Data\n",
    "\n",
    "Let's look at the sample documents we'll be indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3fb45f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Sample Documents to Index (5 total):\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc-001</td>\n",
       "      <td>Introduction to Azure AI Search</td>\n",
       "      <td>Azure Documentation Team</td>\n",
       "      <td>Documentation</td>\n",
       "      <td>azure, search, ai, introduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc-002</td>\n",
       "      <td>Creating Search Indexes</td>\n",
       "      <td>Search Engineering Team</td>\n",
       "      <td>Tutorial</td>\n",
       "      <td>indexes, schema, fields, configuration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc-003</td>\n",
       "      <td>Understanding Indexers and Data Sources</td>\n",
       "      <td>Data Integration Team</td>\n",
       "      <td>Tutorial</td>\n",
       "      <td>indexers, data-sources, blob-storage, automation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc-004</td>\n",
       "      <td>Search Query Syntax Guide</td>\n",
       "      <td>Query Processing Team</td>\n",
       "      <td>Reference</td>\n",
       "      <td>queries, lucene, odata, filters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc-005</td>\n",
       "      <td>Security and Access Control</td>\n",
       "      <td>Security Team</td>\n",
       "      <td>Security</td>\n",
       "      <td>security, api-keys, rbac, authentication</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                    title                    author  \\\n",
       "0  doc-001          Introduction to Azure AI Search  Azure Documentation Team   \n",
       "1  doc-002                  Creating Search Indexes   Search Engineering Team   \n",
       "2  doc-003  Understanding Indexers and Data Sources     Data Integration Team   \n",
       "3  doc-004                Search Query Syntax Guide     Query Processing Team   \n",
       "4  doc-005              Security and Access Control             Security Team   \n",
       "\n",
       "        category                                              tags  \n",
       "0  Documentation                   azure, search, ai, introduction  \n",
       "1       Tutorial            indexes, schema, fields, configuration  \n",
       "2       Tutorial  indexers, data-sources, blob-storage, automation  \n",
       "3      Reference                   queries, lucene, odata, filters  \n",
       "4       Security          security, api-keys, rbac, authentication  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display sample data\n",
    "sample_docs = []\n",
    "\n",
    "for json_file in sorted(DATA_PATH.glob(\"*.json\")):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    # Read associated content file\n",
    "    content_file = DATA_PATH / metadata.get('contentFile', '')\n",
    "    content = \"\"\n",
    "    if content_file.exists():\n",
    "        with open(content_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()[:200] + \"...\"  # First 200 chars\n",
    "    \n",
    "    sample_docs.append({\n",
    "        'id': metadata['id'],\n",
    "        'title': metadata['title'],\n",
    "        'author': metadata['author'],\n",
    "        'category': metadata['category'],\n",
    "        'tags': ', '.join(metadata.get('tags', [])),\n",
    "        'content_preview': content\n",
    "    })\n",
    "\n",
    "# Display as DataFrame\n",
    "df = pd.DataFrame(sample_docs)\n",
    "print(f\"üìö Sample Documents to Index ({len(sample_docs)} total):\\n\")\n",
    "display(df[['id', 'title', 'author', 'category', 'tags']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fdb9ae",
   "metadata": {},
   "source": [
    "## 4. Create Search Index\n",
    "\n",
    "Define the index schema with fields matching our document structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63fd0786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Index 'indexer-test-docs' created/updated successfully!\n",
      "   Fields: 8\n",
      "   - id: Edm.String (key=True, searchable=False)\n",
      "   - title: Edm.String (key=False, searchable=True)\n",
      "   - author: Edm.String (key=False, searchable=True)\n",
      "   - content: Edm.String (key=False, searchable=True)\n",
      "   - category: Edm.String (key=False, searchable=False)\n",
      "   - language: Edm.String (key=False, searchable=False)\n",
      "   - createdDate: Edm.DateTimeOffset (key=False, searchable=False)\n",
      "   - tags: Collection(Edm.String) (key=False, searchable=True)\n"
     ]
    }
   ],
   "source": [
    "# Define the index schema\n",
    "index = SearchIndex(\n",
    "    name=INDEX_NAME,\n",
    "    fields=[\n",
    "        # Key field (required)\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "        \n",
    "        # Searchable text fields\n",
    "        SearchableField(name=\"title\", type=SearchFieldDataType.String, \n",
    "                       sortable=True, filterable=True),\n",
    "        SearchableField(name=\"author\", type=SearchFieldDataType.String,\n",
    "                       filterable=True, facetable=True),\n",
    "        SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "        \n",
    "        # Filterable/Facetable fields\n",
    "        SimpleField(name=\"category\", type=SearchFieldDataType.String,\n",
    "                   filterable=True, facetable=True, sortable=True),\n",
    "        SimpleField(name=\"language\", type=SearchFieldDataType.String,\n",
    "                   filterable=True, facetable=True),\n",
    "        \n",
    "        # Date field\n",
    "        SimpleField(name=\"createdDate\", type=SearchFieldDataType.DateTimeOffset,\n",
    "                   filterable=True, sortable=True),\n",
    "        \n",
    "        # Collection field for tags\n",
    "        SearchField(name=\"tags\", type=SearchFieldDataType.Collection(SearchFieldDataType.String),\n",
    "                   searchable=True, filterable=True, facetable=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create or update the index\n",
    "try:\n",
    "    result = index_client.create_or_update_index(index)\n",
    "    print(f\"‚úÖ Index '{result.name}' created/updated successfully!\")\n",
    "    print(f\"   Fields: {len(result.fields)}\")\n",
    "    for field in result.fields:\n",
    "        print(f\"   - {field.name}: {field.type} (key={field.key}, searchable={field.searchable})\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating index: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc89da84",
   "metadata": {},
   "source": [
    "## 5. Create Data Source Connection\n",
    "\n",
    "Configure a data source pointing to our local file system with JSON documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df53d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data source 'local-test-files' created/updated successfully!\n",
      "   Type: filesystem\n",
      "   Path: C:\\Projets\\AzureAISimulator\\samples\\IndexerTestNotebook\\data\n"
     ]
    }
   ],
   "source": [
    "# Create a data source connection pointing to local files\n",
    "# The simulator supports \"filesystem\" type for local development\n",
    "# container.name is combined with connection_string as a subfolder\n",
    "# Use \".\" for root (no subfolder) since our files are directly in DATA_PATH\n",
    "\n",
    "data_source = SearchIndexerDataSourceConnection(\n",
    "    name=DATA_SOURCE_NAME,\n",
    "    type=\"filesystem\",  # Simulator-specific type for local files\n",
    "    connection_string=str(DATA_PATH),\n",
    "    container=SearchIndexerDataContainer(name=\".\", query=\"*.json\")  # \".\" means root, query filters to *.json files\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = indexer_client.create_or_update_data_source_connection(data_source)\n",
    "    print(f\"‚úÖ Data source '{result.name}' created/updated successfully!\")\n",
    "    print(f\"   Type: {result.type}\")\n",
    "    print(f\"   Path: {result.connection_string}\")\n",
    "    print(f\"   Container: {result.container.name if result.container else 'N/A'}\")\n",
    "    print(f\"   Query: {result.container.query if result.container else 'N/A'}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating data source: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f95ffa",
   "metadata": {},
   "source": [
    "## 6. Create and Run Indexer\n",
    "\n",
    "Create an indexer that processes the JSON files and maps fields to the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2415c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Indexer 'test-indexer' created/updated successfully!\n",
      "   Data Source: local-test-files\n",
      "   Target Index: indexer-test-docs\n"
     ]
    }
   ],
   "source": [
    "# Create an indexer with JSON parsing configuration\n",
    "indexer = SearchIndexer(\n",
    "    name=INDEXER_NAME,\n",
    "    data_source_name=DATA_SOURCE_NAME,\n",
    "    target_index_name=INDEX_NAME,\n",
    "    parameters=IndexingParameters(\n",
    "        configuration=IndexingParametersConfiguration(\n",
    "            parsing_mode=\"json\"  # Parse JSON documents\n",
    "        )\n",
    "    ),\n",
    "    # Field mappings from JSON to index fields\n",
    "    field_mappings=[\n",
    "        FieldMapping(source_field_name=\"id\", target_field_name=\"id\"),\n",
    "        FieldMapping(source_field_name=\"title\", target_field_name=\"title\"),\n",
    "        FieldMapping(source_field_name=\"author\", target_field_name=\"author\"),\n",
    "        FieldMapping(source_field_name=\"category\", target_field_name=\"category\"),\n",
    "        FieldMapping(source_field_name=\"tags\", target_field_name=\"tags\"),\n",
    "        FieldMapping(source_field_name=\"createdDate\", target_field_name=\"createdDate\"),\n",
    "        FieldMapping(source_field_name=\"language\", target_field_name=\"language\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = indexer_client.create_or_update_indexer(indexer)\n",
    "    print(f\"‚úÖ Indexer '{result.name}' created/updated successfully!\")\n",
    "    print(f\"   Data Source: {result.data_source_name}\")\n",
    "    print(f\"   Target Index: {result.target_index_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating indexer: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc7fc528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running indexer...\n",
      "‚úÖ Indexer run triggered!\n",
      "\n",
      "‚è≥ Waiting for indexer to complete...\n",
      "   Status: success\n",
      "\n",
      "üìä Indexer Execution Results:\n",
      "   Status: success\n",
      "   Items Processed: 0\n",
      "   Items Failed: 0\n"
     ]
    }
   ],
   "source": [
    "# Run the indexer\n",
    "print(\"üöÄ Running indexer...\")\n",
    "try:\n",
    "    indexer_client.run_indexer(INDEXER_NAME)\n",
    "    print(\"‚úÖ Indexer run triggered!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running indexer: {e}\")\n",
    "\n",
    "# Wait for indexer to complete\n",
    "print(\"\\n‚è≥ Waiting for indexer to complete...\")\n",
    "max_wait = 30  # seconds\n",
    "wait_interval = 2\n",
    "\n",
    "for i in range(0, max_wait, wait_interval):\n",
    "    time.sleep(wait_interval)\n",
    "    try:\n",
    "        status = indexer_client.get_indexer_status(INDEXER_NAME)\n",
    "        last_result = status.last_result\n",
    "        \n",
    "        if last_result:\n",
    "            print(f\"   Status: {last_result.status}\")\n",
    "            if last_result.status in [\"success\", \"transientFailure\", \"reset\"]:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"   Checking status... ({e})\")\n",
    "\n",
    "# Get final status\n",
    "status = indexer_client.get_indexer_status(INDEXER_NAME)\n",
    "if status.last_result:\n",
    "    result = status.last_result\n",
    "    print(f\"\\nüìä Indexer Execution Results:\")\n",
    "    print(f\"   Status: {result.status}\")\n",
    "    print(f\"   Items Processed: {result.item_count}\")\n",
    "    print(f\"   Items Failed: {result.failed_item_count}\")\n",
    "    if result.errors:\n",
    "        print(f\"   Errors:\")\n",
    "        for error in result.errors:\n",
    "            print(f\"      - {error.error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55a5a33",
   "metadata": {},
   "source": [
    "## 7. Verify Indexed Documents\n",
    "\n",
    "Check that all documents were indexed correctly by querying the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3c9771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create search client to query the index\n",
    "search_client = SearchClient(\n",
    "    endpoint=SEARCH_ENDPOINT,\n",
    "    index_name=INDEX_NAME,\n",
    "    credential=admin_credential,\n",
    "    transport=transport,\n",
    "    connection_verify=False\n",
    ")\n",
    "\n",
    "# Get document count\n",
    "results = search_client.search(search_text=\"*\", include_total_count=True)\n",
    "results_list = list(results)\n",
    "\n",
    "print(f\"üìä Document Count Verification:\")\n",
    "print(f\"   Expected: 5 documents\")\n",
    "print(f\"   Actual:   {len(results_list)} documents\")\n",
    "\n",
    "if len(results_list) == 5:\n",
    "    print(\"   ‚úÖ All documents indexed successfully!\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Document count mismatch!\")\n",
    "\n",
    "# Display all indexed documents\n",
    "print(f\"\\nüìö Indexed Documents:\")\n",
    "doc_data = []\n",
    "for doc in results_list:\n",
    "    doc_data.append({\n",
    "        'id': doc.get('id'),\n",
    "        'title': doc.get('title'),\n",
    "        'author': doc.get('author'),\n",
    "        'category': doc.get('category'),\n",
    "        'tags': ', '.join(doc.get('tags', []) or [])\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(doc_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c555baf1",
   "metadata": {},
   "source": [
    "## 8. Test Search Functionality\n",
    "\n",
    "Verify that the indexed content is searchable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc0363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test search with different queries\n",
    "test_queries = [\n",
    "    (\"indexer\", \"Should find doc about indexers\"),\n",
    "    (\"security\", \"Should find security document\"),\n",
    "    (\"Azure\", \"Should find multiple documents\"),\n",
    "]\n",
    "\n",
    "print(\"üîç Search Tests:\\n\")\n",
    "for query, description in test_queries:\n",
    "    results = search_client.search(search_text=query, top=5)\n",
    "    results_list = list(results)\n",
    "    \n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"Description: {description}\")\n",
    "    print(f\"Results: {len(results_list)} document(s)\")\n",
    "    \n",
    "    for doc in results_list:\n",
    "        print(f\"   - [{doc.get('id')}] {doc.get('title')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1bd114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test filtering by category\n",
    "print(\"üè∑Ô∏è Filter Tests:\\n\")\n",
    "\n",
    "# Filter by category\n",
    "results = search_client.search(\n",
    "    search_text=\"*\", \n",
    "    filter=\"category eq 'Tutorial'\"\n",
    ")\n",
    "tutorial_docs = list(results)\n",
    "print(f\"Category = 'Tutorial': {len(tutorial_docs)} document(s)\")\n",
    "for doc in tutorial_docs:\n",
    "    print(f\"   - {doc.get('title')}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Get facets by category\n",
    "results = search_client.search(\n",
    "    search_text=\"*\", \n",
    "    facets=[\"category\", \"author\"]\n",
    ")\n",
    "results_list = list(results)\n",
    "\n",
    "print(\"üìä Facet Results:\")\n",
    "facets = results.get_facets()\n",
    "if facets:\n",
    "    for facet_name, facet_values in facets.items():\n",
    "        print(f\"\\n{facet_name}:\")\n",
    "        for fv in facet_values:\n",
    "            print(f\"   - {fv.get('value')}: {fv.get('count')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b930e5be",
   "metadata": {},
   "source": [
    "## 9. Cleanup (Optional)\n",
    "\n",
    "Delete all resources created during this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e51250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell to clean up all resources\n",
    "# WARNING: This will delete the index, indexer, and data source!\n",
    "\n",
    "cleanup = False  # Set to True to enable cleanup\n",
    "\n",
    "if cleanup:\n",
    "    print(\"üßπ Cleaning up resources...\")\n",
    "    \n",
    "    # Delete indexer first\n",
    "    try:\n",
    "        indexer_client.delete_indexer(INDEXER_NAME)\n",
    "        print(f\"   ‚úÖ Deleted indexer: {INDEXER_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Could not delete indexer: {e}\")\n",
    "    \n",
    "    # Delete data source\n",
    "    try:\n",
    "        indexer_client.delete_data_source_connection(DATA_SOURCE_NAME)\n",
    "        print(f\"   ‚úÖ Deleted data source: {DATA_SOURCE_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Could not delete data source: {e}\")\n",
    "    \n",
    "    # Delete index\n",
    "    try:\n",
    "        index_client.delete_index(INDEX_NAME)\n",
    "        print(f\"   ‚úÖ Deleted index: {INDEX_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Could not delete index: {e}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Cleanup complete!\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Cleanup skipped. Set cleanup = True to delete resources.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fdddc5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "| Feature | Status |\n",
    "|---------|--------|\n",
    "| Index Creation | ‚úÖ Created index with 8 fields |\n",
    "| Data Source | ‚úÖ Configured local file system data source |\n",
    "| Indexer | ‚úÖ Created and executed indexer |\n",
    "| Document Indexing | ‚úÖ Indexed 5 documents |\n",
    "| Search | ‚úÖ Full-text search working |\n",
    "| Filtering | ‚úÖ OData filters working |\n",
    "| Faceting | ‚úÖ Faceted navigation working |\n",
    "\n",
    "The Azure AI Search Simulator successfully replicates the core indexer functionality of Azure AI Search!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
