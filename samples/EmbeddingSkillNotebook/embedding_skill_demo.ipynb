{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7af4bdd2",
   "metadata": {},
   "source": [
    "# Azure AI Search Simulator - Embedding Skill Demo\n",
    "\n",
    "This notebook demonstrates how to use the **Azure OpenAI Embedding Skill** with the Azure AI Search Simulator to generate vector embeddings for semantic search.\n",
    "\n",
    "## What This Notebook Covers\n",
    "\n",
    "1. **Vector Index Creation** - Create an index with vector fields for embeddings\n",
    "2. **Skillset with Embedding Skill** - Configure Azure OpenAI Embedding skill\n",
    "3. **Indexer with Enrichment** - Process documents and generate embeddings\n",
    "4. **Vector Search** - Perform similarity search using embeddings\n",
    "5. **Hybrid Search** - Combine keyword and vector search with RRF fusion\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Configure your `.env` file** in the workspace root:\n",
    "\n",
    "   ```bash\n",
    "   cp .env.example .env\n",
    "   ```\n",
    "   Then fill in the Azure OpenAI variables:\n",
    "   ```dotenv\n",
    "   AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com\n",
    "   AZURE_OPENAI_API_KEY=your-api-key\n",
    "   AZURE_OPENAI_DEPLOYMENT=text-embedding-3-small\n",
    "   ```\n",
    "\n",
    "2. **Start the Azure AI Search Simulator with HTTPS**:\n",
    "\n",
    "   ```bash\n",
    "   cd src/AzureAISearchSimulator.Api\n",
    "   dotnet run --urls \"https://localhost:7250\"\n",
    "   ```\n",
    "\n",
    "3. **Sample data files** are located in `../IndexerTestNotebook/data`\n",
    "\n",
    "> ‚ö†Ô∏è **Note**: The Azure SDK requires HTTPS. The simulator must run on `https://localhost:7250`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705977cb",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea13a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install azure-search-documents requests pandas numpy python-dotenv\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import urllib3\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Azure AI Search SDK imports\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchIndexer,\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexerDataSourceConnection,\n",
    "    SearchIndexerSkillset,\n",
    "    InputFieldMappingEntry,\n",
    "    OutputFieldMappingEntry,\n",
    "    FieldMapping,\n",
    "    IndexingParameters,\n",
    "    IndexingParametersConfiguration,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    ")\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "\n",
    "# For displaying results\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Suppress SSL warnings for local development\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Load environment variables from workspace root .env file\n",
    "env_path = Path(\"../../.env\")\n",
    "if env_path.exists():\n",
    "    load_dotenv(dotenv_path=env_path)\n",
    "    print(f\"‚úÖ Loaded .env from {env_path.resolve()}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No .env file found at {env_path.resolve()}\")\n",
    "    print(\"   Copy .env.example to .env in the workspace root and fill in your values.\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61d2677",
   "metadata": {},
   "source": [
    "## 2. Initialize Azure AI Search Clients\n",
    "\n",
    "Configure connection to the local Azure AI Search Simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e06776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Azure AI Search Simulator\n",
    "# Values are loaded from the workspace root .env file (see .env.example)\n",
    "SEARCH_ENDPOINT = os.getenv(\"BASE_URL\", \"https://localhost:7250\")\n",
    "ADMIN_API_KEY = os.getenv(\"ADMIN_KEY\", \"admin-key-12345\")\n",
    "\n",
    "# Azure OpenAI Configuration (from .env)\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"\")\n",
    "AZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", \"text-embedding-3-small\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\", \"\")\n",
    "\n",
    "if not AZURE_OPENAI_ENDPOINT or not AZURE_OPENAI_API_KEY:\n",
    "    print(\"‚ö†Ô∏è WARNING: AZURE_OPENAI_ENDPOINT and/or AZURE_OPENAI_API_KEY are not set!\")\n",
    "    print(\"   Copy .env.example to .env in the workspace root and fill in your Azure OpenAI values.\")\n",
    "    print(\"   Required variables: AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "# Resource names for this demo\n",
    "INDEX_NAME = \"embedding-demo-docs\"\n",
    "DATA_SOURCE_NAME = \"embedding-demo-files\"\n",
    "SKILLSET_NAME = \"embedding-skillset\"\n",
    "INDEXER_NAME = \"embedding-indexer\"\n",
    "\n",
    "# Path to sample data (from IndexerTestNotebook)\n",
    "DATA_PATH = Path(\"../IndexerTestNotebook/data\").resolve()\n",
    "\n",
    "# Embedding dimensions (text-embedding-3-small uses 1536)\n",
    "EMBEDDING_DIMENSIONS = 1536\n",
    "\n",
    "# Create credentials\n",
    "admin_credential = AzureKeyCredential(ADMIN_API_KEY)\n",
    "\n",
    "# Configure HTTP transport to skip SSL certificate validation for local development\n",
    "import requests as req_lib\n",
    "from azure.core.pipeline.transport import RequestsTransport\n",
    "\n",
    "session = req_lib.Session()\n",
    "session.verify = False\n",
    "transport = RequestsTransport(session=session, connection_verify=False)\n",
    "\n",
    "# Create clients\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=SEARCH_ENDPOINT,\n",
    "    credential=admin_credential,\n",
    "    transport=transport,\n",
    "    connection_verify=False\n",
    ")\n",
    "\n",
    "indexer_client = SearchIndexerClient(\n",
    "    endpoint=SEARCH_ENDPOINT,\n",
    "    credential=admin_credential,\n",
    "    transport=transport,\n",
    "    connection_verify=False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Connected to Azure AI Search Simulator at {SEARCH_ENDPOINT}\")\n",
    "print(f\"üîë Azure OpenAI endpoint: {AZURE_OPENAI_ENDPOINT}\")\n",
    "print(f\"üß† Embedding deployment: {AZURE_OPENAI_DEPLOYMENT}\")\n",
    "print(f\"üìÅ Data path: {DATA_PATH}\")\n",
    "\n",
    "# List sample data files\n",
    "if DATA_PATH.exists():\n",
    "    json_files = list(DATA_PATH.glob(\"*.json\"))\n",
    "    txt_files = list(DATA_PATH.glob(\"*.txt\"))\n",
    "    print(f\"üìÑ Found {len(json_files)} JSON metadata files\")\n",
    "    print(f\"üìÑ Found {len(txt_files)} TXT content files\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Data path not found. Make sure IndexerTestNotebook/data exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06dfa64",
   "metadata": {},
   "source": [
    "## 3. Review Sample Data\n",
    "\n",
    "Let's look at the sample documents we'll be indexing with embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a276aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display sample data\n",
    "sample_docs = []\n",
    "\n",
    "for json_file in sorted(DATA_PATH.glob(\"*.json\")):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    # Read associated content file\n",
    "    content_file = DATA_PATH / metadata.get('contentFile', '')\n",
    "    content = \"\"\n",
    "    if content_file.exists():\n",
    "        with open(content_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "    \n",
    "    sample_docs.append({\n",
    "        'id': metadata['id'],\n",
    "        'title': metadata['title'],\n",
    "        'author': metadata['author'],\n",
    "        'category': metadata['category'],\n",
    "        'content': content,\n",
    "        'content_preview': content[:150] + \"...\" if len(content) > 150 else content\n",
    "    })\n",
    "\n",
    "# Display as DataFrame\n",
    "df = pd.DataFrame(sample_docs)\n",
    "print(f\"üìö Sample Documents to Index ({len(sample_docs)} total):\\n\")\n",
    "display(df[['id', 'title', 'author', 'category', 'content_preview']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc6dc97",
   "metadata": {},
   "source": [
    "## 4. Create Search Index with Vector Field\n",
    "\n",
    "Define an index schema that includes a vector field for embeddings. We'll use HNSW algorithm for efficient vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1664c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vector search configuration\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"hnsw-config\",\n",
    "            parameters={\n",
    "                \"m\": 4,  # Number of bi-directional links\n",
    "                \"efConstruction\": 400,  # Size of dynamic candidate list during indexing\n",
    "                \"efSearch\": 500,  # Size of dynamic candidate list during search\n",
    "                \"metric\": \"cosine\"  # Distance metric\n",
    "            }\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"vector-profile\",\n",
    "            algorithm_configuration_name=\"hnsw-config\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the index schema with vector field\n",
    "index = SearchIndex(\n",
    "    name=INDEX_NAME,\n",
    "    fields=[\n",
    "        # Key field (required)\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "        \n",
    "        # Searchable text fields\n",
    "        SearchableField(name=\"title\", type=SearchFieldDataType.String,\n",
    "                       sortable=True, filterable=True),\n",
    "        SearchableField(name=\"author\", type=SearchFieldDataType.String,\n",
    "                       filterable=True, facetable=True),\n",
    "        SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "        \n",
    "        # Filterable/Facetable fields\n",
    "        SimpleField(name=\"category\", type=SearchFieldDataType.String,\n",
    "                   filterable=True, facetable=True, sortable=True),\n",
    "        SimpleField(name=\"language\", type=SearchFieldDataType.String,\n",
    "                   filterable=True, facetable=True),\n",
    "        \n",
    "        # Date field\n",
    "        SimpleField(name=\"createdDate\", type=SearchFieldDataType.DateTimeOffset,\n",
    "                   filterable=True, sortable=True),\n",
    "        \n",
    "        # Collection field for tags\n",
    "        SearchField(name=\"tags\", type=SearchFieldDataType.Collection(SearchFieldDataType.String),\n",
    "                   searchable=True, filterable=True, facetable=True),\n",
    "        \n",
    "        # Vector field for embeddings\n",
    "        SearchField(\n",
    "            name=\"contentVector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            vector_search_dimensions=EMBEDDING_DIMENSIONS,\n",
    "            vector_search_profile_name=\"vector-profile\"\n",
    "        ),\n",
    "    ],\n",
    "    vector_search=vector_search\n",
    ")\n",
    "\n",
    "# Create or update the index\n",
    "try:\n",
    "    result = index_client.create_or_update_index(index)\n",
    "    print(f\"‚úÖ Index '{result.name}' created/updated successfully!\")\n",
    "    print(f\"   Fields: {len(result.fields)}\")\n",
    "    for field in result.fields:\n",
    "        vector_info = f\", dims={field.vector_search_dimensions}\" if field.vector_search_dimensions else \"\"\n",
    "        print(f\"   - {field.name}: {field.type} (key={field.key}, searchable={field.searchable}{vector_info})\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating index: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8f4be4",
   "metadata": {},
   "source": [
    "## 5. Create Data Source Connection\n",
    "\n",
    "Configure a data source pointing to the sample JSON documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbe867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data source connection pointing to local files\n",
    "data_source = SearchIndexerDataSourceConnection(\n",
    "    name=DATA_SOURCE_NAME,\n",
    "    type=\"filesystem\",  # Simulator-specific type for local files\n",
    "    connection_string=str(DATA_PATH),\n",
    "    container=SearchIndexerDataContainer(name=\".\", query=\"*.json\")\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = indexer_client.create_or_update_data_source_connection(data_source)\n",
    "    print(f\"‚úÖ Data source '{result.name}' created/updated successfully!\")\n",
    "    print(f\"   Type: {result.type}\")\n",
    "    print(f\"   Path: {result.connection_string}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating data source: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa176e1",
   "metadata": {},
   "source": [
    "## 6. Create Skillset with Azure OpenAI Embedding Skill\n",
    "\n",
    "Configure a skillset that uses Azure OpenAI to generate embeddings for the document content.\n",
    "\n",
    "> **Note**: The Azure OpenAI credentials are loaded from the workspace root `.env` file. Make sure `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY`, and `AZURE_OPENAI_DEPLOYMENT` are set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1f3e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create skillset with Azure OpenAI Embedding skill using REST API\n",
    "# The Azure SDK doesn't have direct support for AzureOpenAIEmbeddingSkill,\n",
    "# so we'll use the REST API directly\n",
    "\n",
    "if not AZURE_OPENAI_API_KEY or not AZURE_OPENAI_ENDPOINT:\n",
    "    print(\"‚ö†Ô∏è WARNING: Azure OpenAI credentials are not configured in .env!\")\n",
    "    print(\"   Set AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY in the workspace root .env file.\")\n",
    "\n",
    "skillset_payload = {\n",
    "    \"name\": SKILLSET_NAME,\n",
    "    \"description\": \"Skillset to generate embeddings using Azure OpenAI\",\n",
    "    \"skills\": [\n",
    "        {\n",
    "            \"@odata.type\": \"#Microsoft.Skills.Text.AzureOpenAIEmbeddingSkill\",\n",
    "            \"name\": \"embedding-skill\",\n",
    "            \"description\": \"Generate embeddings for document title\",\n",
    "            \"context\": \"/document\",\n",
    "            \"resourceUri\": AZURE_OPENAI_ENDPOINT,\n",
    "            \"deploymentId\": AZURE_OPENAI_DEPLOYMENT,\n",
    "            \"apiKey\": AZURE_OPENAI_API_KEY,  # API key loaded from .env\n",
    "            \"modelName\": \"text-embedding-3-small\",\n",
    "            \"inputs\": [\n",
    "                {\n",
    "                    \"name\": \"text\",\n",
    "                    \"source\": \"/document/title\"\n",
    "                }\n",
    "            ],\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"name\": \"embedding\",\n",
    "                    \"targetName\": \"contentEmbedding\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create skillset using REST API\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": ADMIN_API_KEY\n",
    "}\n",
    "\n",
    "url = f\"{SEARCH_ENDPOINT}/skillsets/{SKILLSET_NAME}?api-version=2024-07-01\"\n",
    "\n",
    "try:\n",
    "    response = session.put(url, headers=headers, json=skillset_payload)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    print(f\"‚úÖ Skillset '{result['name']}' created/updated successfully!\")\n",
    "    print(f\"   Skills: {len(result['skills'])}\")\n",
    "    for skill in result['skills']:\n",
    "        print(f\"   - {skill['name']}: {skill['@odata.type']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating skillset: {e}\")\n",
    "    if hasattr(e, 'response') and e.response is not None:\n",
    "        print(f\"   Response: {e.response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f948d",
   "metadata": {},
   "source": [
    "## 7. Create and Run Indexer with Skillset\n",
    "\n",
    "Create an indexer that processes documents, generates embeddings using the skillset, and indexes everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a854167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create indexer with skillset using REST API\n",
    "indexer_payload = {\n",
    "    \"name\": INDEXER_NAME,\n",
    "    \"dataSourceName\": DATA_SOURCE_NAME,\n",
    "    \"targetIndexName\": INDEX_NAME,\n",
    "    \"skillsetName\": SKILLSET_NAME,\n",
    "    \"parameters\": {\n",
    "        \"configuration\": {\n",
    "            \"parsingMode\": \"json\"\n",
    "        }\n",
    "    },\n",
    "    \"fieldMappings\": [\n",
    "        {\"sourceFieldName\": \"id\", \"targetFieldName\": \"id\"},\n",
    "        {\"sourceFieldName\": \"title\", \"targetFieldName\": \"title\"},\n",
    "        {\"sourceFieldName\": \"author\", \"targetFieldName\": \"author\"},\n",
    "        {\"sourceFieldName\": \"category\", \"targetFieldName\": \"category\"},\n",
    "        {\"sourceFieldName\": \"tags\", \"targetFieldName\": \"tags\"},\n",
    "        {\"sourceFieldName\": \"createdDate\", \"targetFieldName\": \"createdDate\"},\n",
    "        {\"sourceFieldName\": \"language\", \"targetFieldName\": \"language\"}\n",
    "    ],\n",
    "    \"outputFieldMappings\": [\n",
    "        {\n",
    "            \"sourceFieldName\": \"/document/contentEmbedding\",\n",
    "            \"targetFieldName\": \"contentVector\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "url = f\"{SEARCH_ENDPOINT}/indexers/{INDEXER_NAME}?api-version=2024-07-01\"\n",
    "\n",
    "try:\n",
    "    response = session.put(url, headers=headers, json=indexer_payload)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    print(f\"‚úÖ Indexer '{result['name']}' created/updated successfully!\")\n",
    "    print(f\"   Data Source: {result['dataSourceName']}\")\n",
    "    print(f\"   Target Index: {result['targetIndexName']}\")\n",
    "    print(f\"   Skillset: {result['skillsetName']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating indexer: {e}\")\n",
    "    if hasattr(e, 'response') and e.response is not None:\n",
    "        print(f\"   Response: {e.response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972bdaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset and run the indexer to reprocess all documents\n",
    "print(\"üîÑ Resetting indexer to reprocess all documents...\")\n",
    "\n",
    "try:\n",
    "    reset_url = f\"{SEARCH_ENDPOINT}/indexers/{INDEXER_NAME}/reset?api-version=2024-07-01\"\n",
    "    response = session.post(reset_url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    print(\"‚úÖ Indexer reset!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not reset indexer (may not exist yet): {e}\")\n",
    "\n",
    "print(\"\\nüöÄ Running indexer...\")\n",
    "\n",
    "try:\n",
    "    run_url = f\"{SEARCH_ENDPOINT}/indexers/{INDEXER_NAME}/run?api-version=2024-07-01\"\n",
    "    response = session.post(run_url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    print(\"‚úÖ Indexer run triggered!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running indexer: {e}\")\n",
    "\n",
    "# Wait for indexer to complete\n",
    "print(\"\\n‚è≥ Waiting for indexer to complete (this may take a while for embedding generation)...\")\n",
    "max_wait = 90  # seconds - increased for embedding generation\n",
    "wait_interval = 3\n",
    "\n",
    "for i in range(0, max_wait, wait_interval):\n",
    "    time.sleep(wait_interval)\n",
    "    try:\n",
    "        status_url = f\"{SEARCH_ENDPOINT}/indexers/{INDEXER_NAME}/status?api-version=2024-07-01\"\n",
    "        response = session.get(status_url, headers=headers)\n",
    "        status = response.json()\n",
    "        \n",
    "        last_result = status.get('lastResult')\n",
    "        if last_result:\n",
    "            status_val = last_result.get('status', 'unknown')\n",
    "            items = last_result.get('itemsProcessed', 0)\n",
    "            print(f\"   Status: {status_val} (items: {items})\")\n",
    "            if status_val in [\"success\", \"transientFailure\", \"reset\"]:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"   Checking status... ({e})\")\n",
    "\n",
    "# Get final status\n",
    "try:\n",
    "    response = session.get(status_url, headers=headers)\n",
    "    status = response.json()\n",
    "    last_result = status.get('lastResult', {})\n",
    "    \n",
    "    print(f\"\\nüìä Indexer Execution Results:\")\n",
    "    print(f\"   Status: {last_result.get('status', 'unknown')}\")\n",
    "    print(f\"   Items Processed: {last_result.get('itemsProcessed', 0)}\")\n",
    "    print(f\"   Items Failed: {last_result.get('itemsFailed', 0)}\")\n",
    "    \n",
    "    errors = last_result.get('errors', [])\n",
    "    if errors:\n",
    "        print(f\"   Errors:\")\n",
    "        for error in errors[:5]:  # Show first 5 errors\n",
    "            print(f\"      - {error.get('errorMessage', 'Unknown error')}\")\n",
    "    \n",
    "    warnings = last_result.get('warnings', [])\n",
    "    if warnings:\n",
    "        print(f\"   Warnings:\")\n",
    "        for warning in warnings[:3]:\n",
    "            print(f\"      - {warning.get('message', 'Unknown warning')}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error getting status: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7615ad72",
   "metadata": {},
   "source": [
    "## 8. Verify Indexed Documents with Embeddings\n",
    "\n",
    "Check that documents were indexed with their vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448b62db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create search client to query the index\n",
    "search_client = SearchClient(\n",
    "    endpoint=SEARCH_ENDPOINT,\n",
    "    index_name=INDEX_NAME,\n",
    "    credential=admin_credential,\n",
    "    transport=transport,\n",
    "    connection_verify=False\n",
    ")\n",
    "\n",
    "# Get all documents\n",
    "results = search_client.search(search_text=\"*\", include_total_count=True, select=[\"id\", \"title\", \"category\", \"contentVector\"])\n",
    "results_list = list(results)\n",
    "\n",
    "print(f\"üìä Document Count Verification:\")\n",
    "print(f\"   Expected: 5 documents\")\n",
    "print(f\"   Actual:   {len(results_list)} documents\")\n",
    "\n",
    "# Check for embeddings\n",
    "docs_with_vectors = 0\n",
    "for doc in results_list:\n",
    "    vector = doc.get('contentVector')\n",
    "    has_vector = vector is not None and len(vector) > 0\n",
    "    if has_vector:\n",
    "        docs_with_vectors += 1\n",
    "        vector_preview = str(vector[:5]) + \"...\" if len(vector) > 5 else str(vector)\n",
    "        print(f\"   ‚úÖ {doc['id']}: {doc['title']} - Vector dims: {len(vector)}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è {doc['id']}: {doc['title']} - No vector\")\n",
    "\n",
    "print(f\"\\nüìä Documents with embeddings: {docs_with_vectors}/{len(results_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8c49c5",
   "metadata": {},
   "source": [
    "## 9. Vector Search (Semantic Search)\n",
    "\n",
    "Use vector search to find semantically similar documents. We'll generate a query embedding and search for similar content.\n",
    "\n",
    "> **Note**: For this demo, we'll use a sample query vector. In production, you would generate the query embedding using the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ca333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demo purposes, we'll use one of the document vectors as a query vector\n",
    "# In production, you would call Azure OpenAI to generate the query embedding\n",
    "\n",
    "# Get a document vector to use as query\n",
    "sample_doc = results_list[0] if results_list else None\n",
    "if sample_doc and sample_doc.get('contentVector'):\n",
    "    query_vector = sample_doc['contentVector']\n",
    "    print(f\"üîç Using vector from document '{sample_doc['id']}' as query vector\")\n",
    "    print(f\"   Vector dimensions: {len(query_vector)}\")\n",
    "    print(f\"   First 5 values: {query_vector[:5]}\")\n",
    "else:\n",
    "    # Create a random vector as fallback (for demo without Azure OpenAI)\n",
    "    query_vector = np.random.rand(EMBEDDING_DIMENSIONS).astype(float).tolist()\n",
    "    print(f\"üîç Using random vector as query (no documents with vectors found)\")\n",
    "    print(f\"   Vector dimensions: {len(query_vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96860cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform vector search\n",
    "print(\"üîç Vector Search Results:\\n\")\n",
    "\n",
    "try:\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=query_vector,\n",
    "        k_nearest_neighbors=5,\n",
    "        fields=\"contentVector\"\n",
    "    )\n",
    "    \n",
    "    results = search_client.search(\n",
    "        search_text=None,  # Pure vector search\n",
    "        vector_queries=[vector_query],\n",
    "        select=[\"id\", \"title\", \"category\", \"author\"]\n",
    "    )\n",
    "    \n",
    "    results_list = list(results)\n",
    "    \n",
    "    if results_list:\n",
    "        data = []\n",
    "        for i, doc in enumerate(results_list, 1):\n",
    "            score = doc.get('@search.score', 0)\n",
    "            data.append({\n",
    "                'Rank': i,\n",
    "                'ID': doc['id'],\n",
    "                'Title': doc['title'],\n",
    "                'Category': doc.get('category', 'N/A'),\n",
    "                'Score': f\"{score:.4f}\"\n",
    "            })\n",
    "        \n",
    "        display(pd.DataFrame(data))\n",
    "    else:\n",
    "        print(\"No results found. Make sure documents have vectors indexed.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error performing vector search: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ca3a9a",
   "metadata": {},
   "source": [
    "## 10. Hybrid Search (Keyword + Vector)\n",
    "\n",
    "Combine traditional keyword search with vector search for best results. The simulator uses Reciprocal Rank Fusion (RRF) to combine results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74fcb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hybrid search\n",
    "print(\"üîç Hybrid Search Results (keyword + vector):\\n\")\n",
    "print(\"Query: 'Azure search' (text) + vector similarity\\n\")\n",
    "\n",
    "try:\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=query_vector,\n",
    "        k_nearest_neighbors=5,\n",
    "        fields=\"contentVector\"\n",
    "    )\n",
    "    \n",
    "    results = search_client.search(\n",
    "        search_text=\"Azure search\",  # Keyword search\n",
    "        vector_queries=[vector_query],  # Plus vector search\n",
    "        select=[\"id\", \"title\", \"category\", \"author\"],\n",
    "        top=5\n",
    "    )\n",
    "    \n",
    "    results_list = list(results)\n",
    "    \n",
    "    if results_list:\n",
    "        data = []\n",
    "        for i, doc in enumerate(results_list, 1):\n",
    "            score = doc.get('@search.score', 0)\n",
    "            data.append({\n",
    "                'Rank': i,\n",
    "                'ID': doc['id'],\n",
    "                'Title': doc['title'],\n",
    "                'Category': doc.get('category', 'N/A'),\n",
    "                'Hybrid Score': f\"{score:.4f}\"\n",
    "            })\n",
    "        \n",
    "        display(pd.DataFrame(data))\n",
    "        print(\"\\nüí° Hybrid search combines keyword relevance with semantic similarity using RRF fusion.\")\n",
    "    else:\n",
    "        print(\"No results found.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error performing hybrid search: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d78731a",
   "metadata": {},
   "source": [
    "## 11. Compare Search Methods\n",
    "\n",
    "Compare results from keyword-only, vector-only, and hybrid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baad984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different search methods\n",
    "search_query = \"machine learning AI\"\n",
    "\n",
    "print(f\"üîç Comparing search methods for: '{search_query}'\\n\")\n",
    "\n",
    "# 1. Keyword Search\n",
    "print(\"1Ô∏è‚É£ Keyword Search (BM25):\")\n",
    "try:\n",
    "    results = search_client.search(\n",
    "        search_text=search_query,\n",
    "        select=[\"id\", \"title\"],\n",
    "        top=3\n",
    "    )\n",
    "    for doc in results:\n",
    "        print(f\"   - [{doc['id']}] {doc['title']} (score: {doc.get('@search.score', 0):.4f})\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# 2. Vector Search\n",
    "print(\"2Ô∏è‚É£ Vector Search (Semantic):\")\n",
    "try:\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=query_vector,\n",
    "        k_nearest_neighbors=3,\n",
    "        fields=\"contentVector\"\n",
    "    )\n",
    "    results = search_client.search(\n",
    "        search_text=None,\n",
    "        vector_queries=[vector_query],\n",
    "        select=[\"id\", \"title\"],\n",
    "        top=3\n",
    "    )\n",
    "    for doc in results:\n",
    "        print(f\"   - [{doc['id']}] {doc['title']} (score: {doc.get('@search.score', 0):.4f})\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# 3. Hybrid Search\n",
    "print(\"3Ô∏è‚É£ Hybrid Search (RRF Fusion):\")\n",
    "try:\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=query_vector,\n",
    "        k_nearest_neighbors=3,\n",
    "        fields=\"contentVector\"\n",
    "    )\n",
    "    results = search_client.search(\n",
    "        search_text=search_query,\n",
    "        vector_queries=[vector_query],\n",
    "        select=[\"id\", \"title\"],\n",
    "        top=3\n",
    "    )\n",
    "    for doc in results:\n",
    "        print(f\"   - [{doc['id']}] {doc['title']} (score: {doc.get('@search.score', 0):.4f})\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c2b46",
   "metadata": {},
   "source": [
    "## 12. Cleanup (Optional)\n",
    "\n",
    "Delete all resources created during this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a9549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell to clean up all resources\n",
    "# WARNING: This will delete the index, indexer, skillset, and data source!\n",
    "\n",
    "cleanup = False  # Set to True to enable cleanup\n",
    "\n",
    "if cleanup:\n",
    "    print(\"üßπ Cleaning up resources...\")\n",
    "    \n",
    "    # Delete indexer first\n",
    "    try:\n",
    "        url = f\"{SEARCH_ENDPOINT}/indexers/{INDEXER_NAME}?api-version=2024-07-01\"\n",
    "        session.delete(url, headers=headers)\n",
    "        print(f\"   ‚úÖ Deleted indexer: {INDEXER_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Could not delete indexer: {e}\")\n",
    "    \n",
    "    # Delete skillset\n",
    "    try:\n",
    "        url = f\"{SEARCH_ENDPOINT}/skillsets/{SKILLSET_NAME}?api-version=2024-07-01\"\n",
    "        session.delete(url, headers=headers)\n",
    "        print(f\"   ‚úÖ Deleted skillset: {SKILLSET_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Could not delete skillset: {e}\")\n",
    "    \n",
    "    # Delete data source\n",
    "    try:\n",
    "        indexer_client.delete_data_source_connection(DATA_SOURCE_NAME)\n",
    "        print(f\"   ‚úÖ Deleted data source: {DATA_SOURCE_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Could not delete data source: {e}\")\n",
    "    \n",
    "    # Delete index\n",
    "    try:\n",
    "        index_client.delete_index(INDEX_NAME)\n",
    "        print(f\"   ‚úÖ Deleted index: {INDEX_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Could not delete index: {e}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Cleanup complete!\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Cleanup skipped. Set cleanup = True to delete resources.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96685b14",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "| Feature | Status | Notes |\n",
    "|---------|--------|-------|\n",
    "| Vector Index | ‚úÖ | Created index with `Collection(Edm.Single)` vector field |\n",
    "| HNSW Configuration | ‚úÖ | Configured HNSW algorithm for vector search |\n",
    "| Embedding Skillset | ‚úÖ | Azure OpenAI Embedding skill for generating vectors |\n",
    "| Indexer with Skills | ‚úÖ | Processed documents and generated embeddings |\n",
    "| Vector Search | ‚úÖ | Semantic similarity search using embeddings |\n",
    "| Hybrid Search | ‚úÖ | Combined keyword + vector with RRF fusion |\n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "1. **Vector Fields**: Use `Collection(Edm.Single)` with `vector_search_dimensions` for embeddings\n",
    "2. **HNSW Algorithm**: Configure M, efConstruction, and efSearch for performance tuning\n",
    "3. **Embedding Skill**: Azure OpenAI generates 1536-dimensional embeddings (text-embedding-ada-002)\n",
    "4. **Hybrid Search**: RRF fusion combines keyword relevance with semantic similarity\n",
    "5. **Output Field Mappings**: Map skillset outputs (embeddings) to index vector fields\n",
    "\n",
    "The Azure AI Search Simulator successfully replicates the vector search and embedding functionality of Azure AI Search!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
